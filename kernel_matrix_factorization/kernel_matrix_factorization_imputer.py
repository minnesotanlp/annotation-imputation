from imputer import Imputer, pd, Tuple, JSONstr, MatrixDataFrame
from typing import List, Union, Literal
from retrainable_kernel_mf import RetrainableKernelMF
import warnings
from matrix_to_factor_form import matrix_to_factor_form
from factor_to_matrix_form import factor_to_matrix_form
from dataset_formats import FactorDataFrame, MissingDataFrame
from dataset_columns import LABEL_COL, SENTENCE_COL, SENTENCE_ID_COL, USER_ID_COL, FACTOR_FORMAT_COLS
from one_anno_per_cross_split import factor_to_factor_one_anno_per_cross_split as train_val_test_split
import random
import numpy as np
from numba import njit
from sklearn.metrics import mean_squared_error
import json
from tqdm import tqdm
from math import isclose
from datetime import datetime

ITEM_ID_COL = "item_id"
ITEM_COL = "item"

KernelType = Union[Literal['linear'], Literal['sigmoid'], Literal['rbf']]

JSONdict = dict

def get_row_ordering(df: MatrixDataFrame) -> pd.Series:
    ordering: pd.Series = df.iloc[:, 0].copy()
    return ordering
# the decorator is required because the matrix factorization code uses numba
@njit
def njit_set_seed(value):
    random.seed(value)
    np.random.seed(value)

class KernelMatrixFactorizationImputer(Imputer):
    '''Implements kernel matrix factorization (along with a grid search for hyperparameters)'''

    # keys for storing data about the model in the json
    N_FACTORS_KEY = "factors"
    N_EPOCHS_KEY = "epochs"
    KERNEL_KEY = "kernel"
    GAMMA_KEY = "gamma"
    REG_KEY = "reg"
    LR_KEY = "lr"
    INIT_MEAN_KEY = "init_mean"
    INIT_SD_KEY = "init_sd"
    SEED_KEY = "seed"

    # keys for storing data generated by the model in training
    VAL_RMSE_KEY = "val_rmse"
    TEST_RMSE_KEY = "test_rmse"
    IMPUTED_DF_KEY = "imputed_df"
    USER_FEATURES_KEY = "user_features"
    ITEM_FEATURES_KEY = "item_features"
    USER_BIASES_KEY = "user_biases"
    ITEM_BIASES_KEY = "item_biases"

    # key for storing the best model
    BEST_MODEL_KEY = "best_model"

    def __init__(self,
        name="MatrixFactorization",
        n_factors: List[int]=None,
        n_epochs: List[int]=None,
        kernels: List[KernelType]=None,
        gammas: List[Union[Literal['auto'], float]]=None,
        regs: List[float]=None,
        lrs: List[float]=None,
        init_means: List[float]=None,
        init_sds: List[float]=None,
        seeds: List[int]=None,
        bound_ratings=True,
        train_frac=0.9,
        val_frac=0.05,
        test_frac=0.05,
        log_file="matrix_factorization_imputer_log.txt",
        save_imputations=False):
        '''
        We have lists of parameters where the original has just one. Here's the documentation for the original:
        n_factors {int} -- The number of latent factors in matrices P and Q (default: {100})
        n_epochs {int} -- Number of epochs to train for (default: {100})
        kernel {str} -- Kernel function to use between user and item features. Options are 'linear', 'logistic' or 'rbf'. (default: {'linear'})
        gamma {str or float} -- Kernel coefficient for 'rbf'. Ignored by other kernels. If 'auto' is used then will be set to 1/n_factors. (default: 'auto')
        reg {float} -- Regularization parameter lambda for Tikhonov regularization (default: {0.01})
        lr {float} -- Learning rate alpha for gradient optimization step (default: {0.01})
        init_mean {float} -- Mean of normal distribution to use for initializing parameters (default: {0})
        init_sd {float} -- Standard deviation of normal distribution to use for initializing parameters (default: {0.1})

        We also include a train_frac, val_frac, and test_frac to split the data into train, validation, and test sets.
        The model will be trained on the train set and evaluated on the validation set. The best model on the validation set will also be evaluated on the test set.

        We also include seeds, which is a list of seeds to use for the random number generator. This is useful for understanding the impact of the random initialization on the results.

        The bound_ratings should be true if the min and max of the predicted ratings should be bounded by the min and the max of the given training data.

        If save_imputations is true, then the imputed data from each model in the grid search will be saved to the json.
        '''
        if n_factors is None:
            n_factors = [10]
        
        if n_epochs is None:
            n_epochs = [1, 5, 10, 100, 500, 1000, 2000, 5000]

        if kernels is None:
            kernels = ["rbf"]

        if gammas is None:
            gammas = ["auto"]

        if regs is None:
            regs = [0.005]

        if lrs is None:
            lrs = [0.001]

        if init_means is None:
            init_means = [0]

        if init_sds is None:
            init_sds = [0.1]

        if seeds is None:
            seeds = [42]

        super().__init__(name)
        self.n_factors = n_factors
        self.n_epochs = sorted(n_epochs)

        # create a list of how many epochs to train for before stopping and evaluating
        # this is what will be used in the grid search
        epoch_intervals = [self.n_epochs[0]]
        for i in range(len(self.n_epochs) - 1):
            epoch_intervals.append(self.n_epochs[i + 1] - self.n_epochs[i])
        self.epoch_intervals = epoch_intervals
        assert sum(self.epoch_intervals) == self.n_epochs[-1], f"sum of epoch_intervals ({sum(self.epoch_intervals)}) must equal the last element of self.n_epochs ({self.n_epochs[-1]}). Epoch intervals: {self.epoch_intervals}, self.n_epochs: {self.n_epochs}"
        assert min(self.n_epochs) == self.epoch_intervals[0], f"min(self.n_epochs) ({min(self.n_epochs)}) must equal the first element of epoch_intervals ({self.epoch_intervals[0]}). Epoch intervals: {self.epoch_intervals}, self.n_epochs: {self.n_epochs}"

        # grid search properties
        self.kernels = kernels
        self.gammas = gammas
        self.regs = regs
        self.lrs = lrs
        self.init_means = init_means
        self.init_sds = init_sds
        self.seeds = seeds

        # static properties
        self.bound_ratings = bound_ratings

        # training properties
        self.train_frac = train_frac
        self.val_frac = val_frac
        self.test_frac = test_frac
        if self.train_frac + self.val_frac + self.test_frac != 1:
            raise ValueError(f"train_frac + val_frac + test_frac must equal 1 (it was {self.train_frac + self.val_frac + self.test_frac} instead)")
        
        # data/logging properties
        self.log_file = log_file
        self.save_imputations = save_imputations

    def set_seed(self, seed: int):
        random.seed(seed)
        np.random.seed(seed)
        njit_set_seed(seed)

    def log(self, *args, **kwargs):
        # print(*args, **kwargs)
        with open(self.log_file, "a") as f:
            print(*args, **kwargs, file=f)

    def get_imputed_df(self, model: RetrainableKernelMF, orig_df: FactorDataFrame, missing_df: MissingDataFrame, orig_matrix_df: MatrixDataFrame, adjustment: float, keep_row_order=True, keep_column_order=True) -> MatrixDataFrame:
        self.log("Getting imputed data...")
        with warnings.catch_warnings():
            warnings.simplefilter(action='ignore', category=FutureWarning)
            # compute the imputed matrix for this model
            imputed_pred = model.predict(missing_df)
            imputed_df = missing_df.copy()
            imputed_df[LABEL_COL] = imputed_pred
            imputed_df = pd.concat([orig_df, imputed_df])
            imputed_df = imputed_df.rename(columns={ITEM_ID_COL: SENTENCE_ID_COL, ITEM_COL: SENTENCE_COL})
            imputed_df = imputed_df[FACTOR_FORMAT_COLS]
            imputed_df[LABEL_COL] -= adjustment
            imputed_df = factor_to_matrix_form(imputed_df)

            assert imputed_df.columns[0] == SENTENCE_ID_COL, f"The first column ({imputed_df.columns[0]}) must be {SENTENCE_ID_COL}"

            if keep_row_order:
                ordering = get_row_ordering(orig_matrix_df)
                assert set(ordering) == set(imputed_df.iloc[:, 0]), f"The set of examples in the original data must match the set of examples in the imputed data. Original: {set(ordering)}, Imputed: {set(imputed_df.iloc[:, 0])}"
                # make sure the row order matches the original data
                imputed_df = ordering.to_frame().merge(imputed_df, on=SENTENCE_ID_COL, how="left")

            if keep_column_order:
                ordering = orig_matrix_df.columns
                assert set(ordering) == set(imputed_df.columns), f"The set of columns in the original data must match the set of columns in the imputed data. Columns in the original not in the imputed: {set(ordering) - set(imputed_df.columns)}, Columns in the imputed not in the original: {set(imputed_df.columns) - set(ordering)}"
                # make sure the column order matches the original data
                imputed_df = imputed_df[ordering]        

        self.log("Finished imputation.")
        return imputed_df

    def impute(self, df: MatrixDataFrame) -> Tuple[MatrixDataFrame, JSONstr]:
        self.log(f"\n\n\nSTARTED NEW IMPUTATION PROCESS AT {datetime.utcnow()}\n")
        orig_df = df.copy()

        # first, convert the matrix dataframe into factor format, as required by the matrix factorization code
        factor_df: FactorDataFrame
        extra_data: JSONdict = {}
        missing_ratings_df: MissingDataFrame
        factor_df, missing_ratings_df = matrix_to_factor_form(df, get_missing_ratings=True) # type: ignore
        
        # then, make sure all the values are positive, as required for matrix factorization
        adjustment_rating = abs(min([factor_df[LABEL_COL].min(), 0]))
        factor_df[LABEL_COL] += adjustment_rating

        # split the data into train and test
        train_data, val_data, test_data = train_val_test_split(factor_df, train_split=self.train_frac, val_split=self.val_frac)

        # the matrix factorization requires renaming the sentence id and sentence columns to item_id and item
        for df in [factor_df, train_data, val_data, test_data, missing_ratings_df]:
            df.rename(columns={SENTENCE_ID_COL: ITEM_ID_COL, SENTENCE_COL: ITEM_COL}, inplace=True)

        # split the data into x and y
        # x has the user_id and item_id columns
        # y has the label column and is a series
        x_train = train_data[[USER_ID_COL, ITEM_ID_COL]]
        y_train = train_data[LABEL_COL]
        assert isinstance(x_train, pd.DataFrame)
        assert isinstance(y_train, pd.Series)
        x_val = val_data[[USER_ID_COL, ITEM_ID_COL]]
        y_val = val_data[LABEL_COL]
        assert isinstance(x_val, pd.DataFrame)
        assert isinstance(y_val, pd.Series)
        x_test = test_data[[USER_ID_COL, ITEM_ID_COL]]
        y_test = test_data[LABEL_COL]
        assert isinstance(x_test, pd.DataFrame)
        assert isinstance(y_test, pd.Series)

        self.log("Training models...")
        with warnings.catch_warnings():
            warnings.simplefilter(action='ignore', category=FutureWarning)
            # n_factors: List[int]=None,
            # n_epochs: List[int]=None, # needs to be the last one
            # kernels: List[str]=None,
            # gammas: List[Union[str, float]]=None,
            # regs: List[float]=None,
            # lrs: List[float]=None,
            # init_means: List[float]=None,
            # init_sds: List[float]=None,
            # seeds: List[int]=None
            for n_factors in tqdm(self.n_factors, desc="Factors", miniters=1):
                for kernel in tqdm(self.kernels, desc="Kernels", miniters=1, leave=False):
                    for gamma in tqdm(self.gammas, desc="Gammas", miniters=1, leave=False):
                        for reg in tqdm(self.regs, desc="Regs", miniters=1, leave=False):
                            for lr in tqdm(self.lrs, desc="Lrs", miniters=1, leave=False):
                                for init_mean in tqdm(self.init_means, desc="Init means", miniters=1, leave=False):
                                    for init_sd in tqdm(self.init_sds, desc="Init SDs", miniters=1, leave=False):
                                        for seed in tqdm(self.seeds, desc="Seeds", miniters=1, leave=False):
                                            self.set_seed(seed)
                                            # initialize the model
                                            model = RetrainableKernelMF(n_factors=n_factors, 
                                                n_epochs=0,
                                                kernel=kernel,
                                                gamma=gamma,
                                                reg=reg,
                                                lr=lr,
                                                init_mean=init_mean,
                                                init_sd=init_sd,
                                                verbose=0
                                            )
                                            mf_train_data = model.prep_data(x_train, y_train, bound=self.bound_ratings)
                                            model.initialize()
                                            for n_epochs_index, epoch_interval in enumerate(tqdm(self.epoch_intervals, desc="Epochs", miniters=1, leave=False)):
                                                n_epochs = self.n_epochs[n_epochs_index]
                                                model_identifier = f"<Model: factors={n_factors} epochs={n_epochs} kernel={kernel} gamma={gamma} reg={reg} lr={lr} init_mean={init_mean} init_sd={init_sd} seed={seed}>"

                                                # Initial training
                                                model.train(mf_train_data, n_epochs=epoch_interval)

                                                # compute the RMSE on the validation set
                                                y_val_pred = model.predict(x_val)
                                                # squared is false so it's rmse instead of mse
                                                val_rmse = mean_squared_error(y_val, y_val_pred, squared=False)
                                                self.log(f"Validation RMSE:\n    {model_identifier}\n    RMSE: {val_rmse}")

                                                # keys for storing data about the model in the json
                                                # N_FACTORS_KEY = "factors"
                                                # N_EPOCHS_KEY = "epochs"
                                                # KERNEL_KEY = "kernel"
                                                # GAMMA_KEY = "gamma"
                                                # REG_KEY = "reg"
                                                # LR_KEY = "lr"
                                                # INIT_MEAN_KEY = "init_mean"
                                                # INIT_SD_KEY = "init_sd"
                                                # SEED_KEY = "seed"
                                                # VAL_RMSE_KEY = "val_rmse"
                                                extra_data[model_identifier] = {self.N_FACTORS_KEY: n_factors, self.N_EPOCHS_KEY: n_epochs, self.KERNEL_KEY: kernel, self.GAMMA_KEY: gamma, self.REG_KEY: reg, self.LR_KEY: lr, self.INIT_MEAN_KEY: init_mean, self.INIT_SD_KEY: init_sd, self.SEED_KEY: seed, self.VAL_RMSE_KEY: val_rmse}

                                                if self.save_imputations:
                                                    # by default, to_json uses orient='columns'
                                                    imputed_df = self.get_imputed_df(model, factor_df, missing_ratings_df, orig_df, adjustment_rating)
                                                    extra_data[model_identifier][self.IMPUTED_DF_KEY] = imputed_df.to_json()

                                                    # save features and bias as well
                                                    extra_data[model_identifier][self.USER_FEATURES_KEY] = json.dumps(model.user_features.tolist())
                                                    extra_data[model_identifier][self.ITEM_FEATURES_KEY] = json.dumps(model.item_features.tolist())
                                                    extra_data[best_model_identifier][self.USER_BIASES_KEY] = json.dumps(best_model.user_biases.tolist())
                                                    extra_data[best_model_identifier][self.ITEM_BIASES_KEY] = json.dumps(best_model.item_biases.tolist())

        # get the best model and score and number of epochs
        best_model_identifier = min(extra_data, key=lambda model_identifier: extra_data[model_identifier][self.VAL_RMSE_KEY])
        best_rmse = extra_data[best_model_identifier][self.VAL_RMSE_KEY]
        best_factors = extra_data[best_model_identifier][self.N_FACTORS_KEY]
        best_epochs = extra_data[best_model_identifier][self.N_EPOCHS_KEY]
        best_kernel = extra_data[best_model_identifier][self.KERNEL_KEY]
        best_gamma = extra_data[best_model_identifier][self.GAMMA_KEY]
        best_reg = extra_data[best_model_identifier][self.REG_KEY]
        best_lr = extra_data[best_model_identifier][self.LR_KEY]
        best_init_mean = extra_data[best_model_identifier][self.INIT_MEAN_KEY]
        best_init_sd = extra_data[best_model_identifier][self.INIT_SD_KEY]
        best_seed = extra_data[best_model_identifier][self.SEED_KEY]
        extra_data[self.BEST_MODEL_KEY] = best_model_identifier

        # recreate the best model
        self.log(f"The best model was: {best_model_identifier}")
        self.log(f"Recreating the best model...")
        self.set_seed(best_seed)
        best_model = RetrainableKernelMF(n_factors=best_factors, n_epochs=best_epochs, kernel=best_kernel, gamma=best_gamma, reg=best_reg, lr=best_lr, init_mean=best_init_mean, init_sd=best_init_sd, verbose=0)
        best_model.fit(x_train, y_train, bound=self.bound_ratings)
        self.log("Done recreating the best model")

        # recompute the RMSE on the validation set to make sure it's the same
        self.log("Recomputing RMSE on validation set...")
        with warnings.catch_warnings():
            warnings.simplefilter(action='ignore', category=FutureWarning)
            y_val_pred = best_model.predict(x_val)
        # squared is false so it's rmse instead of mse
        val_rmse = mean_squared_error(y_val, y_val_pred, squared=False)
        assert isclose(val_rmse, best_rmse), f"RMSE after retraining should match for {best_model_identifier}. Before retraining: {best_rmse}. After retraining: {val_rmse}. This assert failing indicates a lack of reproducibility/determinism."

        # compute the RMSE on the test set
        self.log("Computing RMSE on test set...")
        with warnings.catch_warnings():
            warnings.simplefilter(action='ignore', category=FutureWarning)
            y_test_pred = best_model.predict(x_test)
        # squared is false so it's rmse instead of mse
        test_rmse = mean_squared_error(y_test, y_test_pred, squared=False)
        self.log("Best model results:")
        self.log(f"Model:\n    {best_model_identifier}")
        self.log(f"Validation RMSE:\n    {val_rmse}")
        self.log(f"Test RMSE:\n    {test_rmse}")

        # save the test RMSE to the extra data
        extra_data[best_model_identifier][self.TEST_RMSE_KEY] = test_rmse

        if self.save_imputations:
            # features should already be in the extra data
            best_imputed_df_str: str = extra_data[best_model_identifier][self.IMPUTED_DF_KEY]
            best_imputed_df: MatrixDataFrame = pd.read_json(best_imputed_df_str, orient='columns')
        else:
            # compute the imputed matrix for the best model
            best_imputed_df = self.get_imputed_df(best_model, factor_df, missing_ratings_df, orig_df, adjustment_rating)

            # add the best features and biases to the extra data
            extra_data[best_model_identifier][self.USER_FEATURES_KEY] = json.dumps(best_model.user_features.tolist())
            extra_data[best_model_identifier][self.ITEM_FEATURES_KEY] = json.dumps(best_model.item_features.tolist())
            extra_data[best_model_identifier][self.USER_BIASES_KEY] = json.dumps(best_model.user_biases.tolist())
            extra_data[best_model_identifier][self.ITEM_BIASES_KEY] = json.dumps(best_model.item_biases.tolist())

        assert isinstance(best_imputed_df, MatrixDataFrame), f"best_imputed_df should be a DataFrame, but is a {type(best_imputed_df)}"

        json_str: JSONstr = json.dumps(extra_data)
        return best_imputed_df, json_str