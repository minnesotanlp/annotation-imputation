In this dataset, annotators are asked whether text is offensive. There are three possible labels: not offensive (0), maybe (1), offensive (2)

Now you will make your prediction (if you are unsure, just give your best estimate.) Your output should be an integer label:
Target Example:
Text: Even when the rest of the world has daggers ready to strike at your heart , you still show them kindness ..
Annotation from annotator:
Your output should be a single integer and nothing else.